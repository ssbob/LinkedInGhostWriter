{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe4d823",
   "metadata": {},
   "source": [
    "# What is this application doing?\n",
    "\n",
    "In the below program, I am reading in a subset of LinkedIn posts by a given author, filtering down to the top X number of most upvoted posts (sorted in descending order). I then take that context, and ask OpenAI (using GPT4 for now, but may see if newest version of GPT4 does any better than the old one, old one, sheesh that's funny to say) to evaluate it and judge style/tone/etc. GPT comes back with 25 ways to measure it, and I then ask GPT4 to evaluate the 25 posts across those 25 properties, to sinmulate to the best of it's ability, the user in question, and write a short trivial post about eating a sandwich in the park. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2832b",
   "metadata": {},
   "source": [
    "### Installing Required Packagages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20bb1-c88d-4b94-b05f-82252f109137",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain python-dotenv openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a517573",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e65bd69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Environment Variables\n",
    "import os\n",
    "\n",
    "# Loading of environment variables (API keys, etc.)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LinkedIn requirements\n",
    "import requests\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9e600",
   "metadata": {},
   "source": [
    "### Setting our OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98123655",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f65ce3",
   "metadata": {},
   "source": [
    "### Creating our LLM Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "063daa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key, model_name='gpt-4', client=\"my_client\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40ecb8",
   "metadata": {},
   "source": [
    "### Definining our Function to get User Posts from LinkedIn\n",
    "\n",
    "In the below cell, I used RapidAPI to featch LinkedIn posts by profile, you could easily swap out something related to Twitter/X, or even substitute reading from a file. Replace the code below with whatever makes sense for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c2b68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_posts(screen_name, posts_to_return):\n",
    "    url = \"https://fresh-linkedin-profile-data.p.rapidapi.com/get-profile-posts\"\n",
    "\n",
    "    querystring = {\"linkedin_url\": f\"https://www.linkedin.com/in/jakemakler/\", \"type\": \"posts\"}   # replace with your LinkedIn URL\n",
    "    \n",
    "    headers = {\n",
    "\t\"X-RapidAPI-Key\": os.getenv('X_RAPIDAPI_KEY'), # replace with your API key\n",
    "\t\"X-RapidAPI-Host\": os.getenv('X_RAPIDAPI_HOST') # replace with your host\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    # load the data into a Python dictionary\n",
    "    data = json.loads(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "    # access the 'data' field (a list of posts)\n",
    "    posts = data['data']\n",
    "\n",
    "    # sort posts by 'num_likes' in descending order\n",
    "    sorted_posts = sorted(posts, key=lambda post: post['num_likes'], reverse=True)\n",
    "\n",
    "    # get the 'text' field of the top 100 posts\n",
    "    texts = [post['text'] for post in sorted_posts[:posts_to_return]]   \n",
    "\n",
    "    # join the texts into a single string separated by two new lines\n",
    "    example_posts = '\\n\\n'.join(texts)\n",
    "    return example_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d599f",
   "metadata": {},
   "source": [
    "### Calling `get_user_posts` for our Target User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_screen_name = 'testuser'  # Replace this with the desired user's screen name\n",
    "example_posts = get_users_posts(user_screen_name, 25) # change the # of example posts to grab\n",
    "print(example_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112a209",
   "metadata": {},
   "source": [
    "### Our First LLM Template\n",
    "\n",
    "This is just trying to mimic without first evaluating the users style, just matching (first level of effort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d25d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Please create me a LinkedIn post about going to the park and eating a sandwich.\n",
    "\n",
    "% TONE\n",
    " - Don't use any emojis or hashtags.\n",
    " - Respond in the tone of <insert name>\n",
    "\n",
    "% START OF EXAMPLE POSTS TO MIMIC\n",
    "{example_posts}\n",
    "% END OF EXAMPLE POSTS TO MIMIC\n",
    "\n",
    "YOUR POST:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"example_posts\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(example_posts=example_posts)\n",
    "\n",
    "print (final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf7f80",
   "metadata": {},
   "source": [
    "### Let's Dig Deeper\n",
    "\n",
    "Asking GPT4 to generate a list of style and tone attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Can you please generate a list of tone attributes and a description to describe a piece of writing by?\n",
    "\n",
    "Things like pace, mood, etc.\n",
    "\n",
    "Respond with nothing else besides the list\n",
    "\"\"\"\n",
    "\n",
    "how_to_describe_tone = llm.predict(prompt)\n",
    "print (how_to_describe_tone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94345252",
   "metadata": {},
   "source": [
    "### Our Function to Get the Authors Tone\n",
    "\n",
    "This function takes in the list of attributes from above, and combines it with our example posts from above, to evaluate the authors tone and style, (level 2 effort) to better mimic the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b1a75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_tone_description(how_to_describe_tone, example_posts):\n",
    "    template = \"\"\"\n",
    "        You are an AI Bot that is very good at generating writing in a similar tone as examples.\n",
    "        Be opinionated and have an active voice.\n",
    "        Take a strong stance with your response.\n",
    "\n",
    "        % HOW TO DESCRIBE TONE\n",
    "        {how_to_describe_tone}\n",
    "\n",
    "        % START OF EXAMPLES\n",
    "        {example_posts}\n",
    "        % END OF EXAMPLES\n",
    "\n",
    "        List out the tone qualities of the examples above\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"how_to_describe_tone\", \"example_posts\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(how_to_describe_tone=how_to_describe_tone, example_posts=example_posts)\n",
    "\n",
    "    authors_tone_description = llm.predict(final_prompt)\n",
    "\n",
    "    return authors_tone_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3160160",
   "metadata": {},
   "source": [
    "### Calling our new Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_tone_description = get_authors_tone_description(how_to_describe_tone, example_posts)\n",
    "print (authors_tone_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df9f67",
   "metadata": {},
   "source": [
    "### This is where things get interesting\n",
    "\n",
    "In this block we are taking it to level 3 effort, mixing our tone/style descriptions, example posts, and some specific prompting to get us the \"ultimate\" ghost writer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a99dee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "% INSTRUCTIONS\n",
    " - You are an AI Bot that is very good at mimicking an authors writing style.\n",
    " - Your goal is to write content with the tone that is described below.\n",
    " - Do not go outside the tone instructions below\n",
    " - Do not use hashtags or emojis\n",
    " - Respond in the tone of Jake Makler\n",
    "\n",
    "% Description of the authors tone:\n",
    "{authors_tone_description}\n",
    "\n",
    "% Authors writing samples\n",
    "{example_posts}\n",
    "\n",
    "% YOUR TASK\n",
    "Please create a LinkedIn about going to the park and eating a sandwich.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"authors_tone_description\", \"example_posts\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(authors_tone_description=authors_tone_description, example_posts=example_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b48094",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
